{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"/run/media/cavani/cavani_hdx/julia-abc/envs/RL\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reset = false\n",
    "\n",
    "PROJECT_DIR = abspath(joinpath(\"..\", \"envs\", \"RL\"))\n",
    "reset && isdir(PROJECT_DIR) && rm(PROJECT_DIR; recursive=true, force=true)\n",
    "\n",
    "PROJECT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m Activating\u001b[22m\u001b[39m environment at `/run/media/cavani/cavani_hdx/julia-abc/envs/RL/Project.toml`\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "Pkg.activate(PROJECT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Updating\u001b[22m\u001b[39m registry at `~/Workspace/julia-abc/software/julia-env/registries/General`\n",
      "\u001b[32m\u001b[1m  Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `/run/media/cavani/cavani_hdx/julia-abc/envs/RL/Project.toml`\n",
      "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `/run/media/cavani/cavani_hdx/julia-abc/envs/RL/Manifest.toml`\n"
     ]
    }
   ],
   "source": [
    "deps = [\n",
    "    \"ReinforcementLearning\",\n",
    "    \"Flux\",\n",
    "    \"CUDA\",\n",
    "    \"Plots\",\n",
    "    \"PyPlot\",\n",
    "]\n",
    "\n",
    "Pkg.add(deps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1mStatus\u001b[22m\u001b[39m `/run/media/cavani/cavani_hdx/julia-abc/envs/RL/Project.toml`\n",
      " \u001b[90m [052768ef] \u001b[39m\u001b[37mCUDA v1.3.3\u001b[39m\n",
      " \u001b[90m [587475ba] \u001b[39m\u001b[37mFlux v0.11.1\u001b[39m\n",
      " \u001b[90m [91a5bcdd] \u001b[39m\u001b[37mPlots v1.6.2\u001b[39m\n",
      " \u001b[90m [d330b81b] \u001b[39m\u001b[37mPyPlot v2.9.0\u001b[39m\n",
      " \u001b[90m [158674fc] \u001b[39m\u001b[37mReinforcementLearning v0.6.0\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "Pkg.status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Updating\u001b[22m\u001b[39m registry at `~/Workspace/julia-abc/software/julia-env/registries/General`\n",
      "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `/run/media/cavani/cavani_hdx/julia-abc/envs/RL/Project.toml`\n",
      "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `/run/media/cavani/cavani_hdx/julia-abc/envs/RL/Manifest.toml`\n",
      "\u001b[32m\u001b[1mPrecompiling\u001b[22m\u001b[39m project...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1mStatus\u001b[22m\u001b[39m `/run/media/cavani/cavani_hdx/julia-abc/envs/RL/Project.toml`\n",
      " \u001b[90m [052768ef] \u001b[39m\u001b[37mCUDA v1.3.3\u001b[39m\n",
      " \u001b[90m [587475ba] \u001b[39m\u001b[37mFlux v0.11.1\u001b[39m\n",
      " \u001b[90m [91a5bcdd] \u001b[39m\u001b[37mPlots v1.6.2\u001b[39m\n",
      " \u001b[90m [d330b81b] \u001b[39m\u001b[37mPyPlot v2.9.0\u001b[39m\n",
      " \u001b[90m [158674fc] \u001b[39m\u001b[37mReinforcementLearning v0.6.0\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "ENV[\"PYTHON\"] = joinpath(ENV[\"CONDA_INSTDIR\"], \"envs\", \"python\", \"bin\", \"python\")\n",
    "Pkg.update()\n",
    "Pkg.API.precompile()\n",
    "Pkg.status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReinforcementLearning\n",
      "361.781874 seconds (79.85 M allocations: 4.266 GiB, 0.69% gc time)\n",
      "PyPlot\n",
      " 11.043227 seconds (5.43 M allocations: 286.153 MiB, 0.94% gc time)\n",
      "Plots\n",
      " 14.799310 seconds (11.45 M allocations: 699.534 MiB, 1.92% gc time)\n",
      "Flux\n",
      " 39.337889 seconds (60.36 M allocations: 3.239 GiB, 4.63% gc time)\n",
      "CUDA\n",
      "  4.067940 seconds (4.63 M allocations: 283.318 MiB, 0.99% gc time)\n"
     ]
    }
   ],
   "source": [
    "JULIA_BIN = joinpath(ENV[\"JULIA_INSTDIR\"], \"bin\", \"julia\")\n",
    "for (_, pkg) in Pkg.dependencies()\n",
    "    pkg.is_direct_dep && pkg.version !== nothing || continue\n",
    "    try\n",
    "        pkg_name = pkg.name\n",
    "        run(`$JULIA_BIN --project=$PROJECT_DIR -e \"println(\\\"$pkg_name\\\"); @time import $pkg_name\"`)\n",
    "    catch\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "This experiment uses three dense layers to approximate the Q value.\n",
       "\\end{verbatim}\n",
       "The testing environment is CartPoleEnv.\n",
       "\n",
       "\\begin{verbatim}\n",
       "Agent and statistic info will be saved to: `/run/media/cavani/cavani_hdx/julia-abc/workspace/checkpoints/JuliaRL_BasicDQN_CartPole_2020_09_02_19_03_20`\n",
       "You can also view the tensorboard logs with\n",
       "`tensorboard --logdir /run/media/cavani/cavani_hdx/julia-abc/workspace/checkpoints/JuliaRL_BasicDQN_CartPole_2020_09_02_19_03_20/tb_log`\n",
       "To load the agent and statistic info:\n",
       "```\n",
       "agent = RLCore.load(\"/run/media/cavani/cavani_hdx/julia-abc/workspace/checkpoints/JuliaRL_BasicDQN_CartPole_2020_09_02_19_03_20\", Agent)\n",
       "BSON.@load joinpath(\"/run/media/cavani/cavani_hdx/julia-abc/workspace/checkpoints/JuliaRL_BasicDQN_CartPole_2020_09_02_19_03_20\", \"stats.bson\") total_reward_per_episode time_per_step\n",
       "```\n",
       "\\end{verbatim}\n"
      ],
      "text/markdown": [
       "```\n",
       "This experiment uses three dense layers to approximate the Q value.\n",
       "```\n",
       "\n",
       "The testing environment is CartPoleEnv.\n",
       "\n",
       "````\n",
       "Agent and statistic info will be saved to: `/run/media/cavani/cavani_hdx/julia-abc/workspace/checkpoints/JuliaRL_BasicDQN_CartPole_2020_09_02_19_03_20`\n",
       "You can also view the tensorboard logs with\n",
       "`tensorboard --logdir /run/media/cavani/cavani_hdx/julia-abc/workspace/checkpoints/JuliaRL_BasicDQN_CartPole_2020_09_02_19_03_20/tb_log`\n",
       "To load the agent and statistic info:\n",
       "```\n",
       "agent = RLCore.load(\"/run/media/cavani/cavani_hdx/julia-abc/workspace/checkpoints/JuliaRL_BasicDQN_CartPole_2020_09_02_19_03_20\", Agent)\n",
       "BSON.@load joinpath(\"/run/media/cavani/cavani_hdx/julia-abc/workspace/checkpoints/JuliaRL_BasicDQN_CartPole_2020_09_02_19_03_20\", \"stats.bson\") total_reward_per_episode time_per_step\n",
       "```\n",
       "````\n"
      ],
      "text/plain": [
       "\u001b[36m  This experiment uses three dense layers to approximate the Q value.\u001b[39m\n",
       "\n",
       "  The testing environment is CartPoleEnv.\n",
       "\n",
       "\u001b[36m  Agent and statistic info will be saved to: `/run/media/cavani/cavani_hdx/julia-abc/workspace/checkpoints/JuliaRL_BasicDQN_CartPole_2020_09_02_19_03_20`\u001b[39m\n",
       "\u001b[36m  You can also view the tensorboard logs with\u001b[39m\n",
       "\u001b[36m  `tensorboard --logdir /run/media/cavani/cavani_hdx/julia-abc/workspace/checkpoints/JuliaRL_BasicDQN_CartPole_2020_09_02_19_03_20/tb_log`\u001b[39m\n",
       "\u001b[36m  To load the agent and statistic info:\u001b[39m\n",
       "\u001b[36m  ```\u001b[39m\n",
       "\u001b[36m  agent = RLCore.load(\"/run/media/cavani/cavani_hdx/julia-abc/workspace/checkpoints/JuliaRL_BasicDQN_CartPole_2020_09_02_19_03_20\", Agent)\u001b[39m\n",
       "\u001b[36m  BSON.@load joinpath(\"/run/media/cavani/cavani_hdx/julia-abc/workspace/checkpoints/JuliaRL_BasicDQN_CartPole_2020_09_02_19_03_20\", \"stats.bson\") total_reward_per_episode time_per_step\u001b[39m\n",
       "\u001b[36m  ```\u001b[39m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:01:41\u001b[39mm\n",
      "┌ Info: saving agent to /run/media/cavani/cavani_hdx/julia-abc/workspace/checkpoints/JuliaRL_BasicDQN_CartPole_2020_09_02_19_03_20 ...\n",
      "└ @ ReinforcementLearningCore /home/cavani/Workspace/julia-abc/software/julia-env/packages/ReinforcementLearningCore/oPAhL/src/components/agents/agent.jl:35\n",
      "┌ Info: finished saving agent in 10.39172259 seconds\n",
      "└ @ ReinforcementLearningCore /home/cavani/Workspace/julia-abc/software/julia-env/packages/ReinforcementLearningCore/oPAhL/src/components/agents/agent.jl:54\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "This experiment uses three dense layers to approximate the Q value.\n",
       "\\end{verbatim}\n",
       "The testing environment is CartPoleEnv.\n",
       "\n",
       "\\begin{verbatim}\n",
       "Agent and statistic info will be saved to: `/run/media/cavani/cavani_hdx/julia-abc/workspace/checkpoints/JuliaRL_BasicDQN_CartPole_2020_09_02_19_03_20`\n",
       "You can also view the tensorboard logs with\n",
       "`tensorboard --logdir /run/media/cavani/cavani_hdx/julia-abc/workspace/checkpoints/JuliaRL_BasicDQN_CartPole_2020_09_02_19_03_20/tb_log`\n",
       "To load the agent and statistic info:\n",
       "```\n",
       "agent = RLCore.load(\"/run/media/cavani/cavani_hdx/julia-abc/workspace/checkpoints/JuliaRL_BasicDQN_CartPole_2020_09_02_19_03_20\", Agent)\n",
       "BSON.@load joinpath(\"/run/media/cavani/cavani_hdx/julia-abc/workspace/checkpoints/JuliaRL_BasicDQN_CartPole_2020_09_02_19_03_20\", \"stats.bson\") total_reward_per_episode time_per_step\n",
       "```\n",
       "\\end{verbatim}\n"
      ],
      "text/markdown": [
       "```\n",
       "This experiment uses three dense layers to approximate the Q value.\n",
       "```\n",
       "\n",
       "The testing environment is CartPoleEnv.\n",
       "\n",
       "````\n",
       "Agent and statistic info will be saved to: `/run/media/cavani/cavani_hdx/julia-abc/workspace/checkpoints/JuliaRL_BasicDQN_CartPole_2020_09_02_19_03_20`\n",
       "You can also view the tensorboard logs with\n",
       "`tensorboard --logdir /run/media/cavani/cavani_hdx/julia-abc/workspace/checkpoints/JuliaRL_BasicDQN_CartPole_2020_09_02_19_03_20/tb_log`\n",
       "To load the agent and statistic info:\n",
       "```\n",
       "agent = RLCore.load(\"/run/media/cavani/cavani_hdx/julia-abc/workspace/checkpoints/JuliaRL_BasicDQN_CartPole_2020_09_02_19_03_20\", Agent)\n",
       "BSON.@load joinpath(\"/run/media/cavani/cavani_hdx/julia-abc/workspace/checkpoints/JuliaRL_BasicDQN_CartPole_2020_09_02_19_03_20\", \"stats.bson\") total_reward_per_episode time_per_step\n",
       "```\n",
       "````\n"
      ],
      "text/plain": [
       "\u001b[36m  This experiment uses three dense layers to approximate the Q value.\u001b[39m\n",
       "\n",
       "  The testing environment is CartPoleEnv.\n",
       "\n",
       "\u001b[36m  Agent and statistic info will be saved to: `/run/media/cavani/cavani_hdx/julia-abc/workspace/checkpoints/JuliaRL_BasicDQN_CartPole_2020_09_02_19_03_20`\u001b[39m\n",
       "\u001b[36m  You can also view the tensorboard logs with\u001b[39m\n",
       "\u001b[36m  `tensorboard --logdir /run/media/cavani/cavani_hdx/julia-abc/workspace/checkpoints/JuliaRL_BasicDQN_CartPole_2020_09_02_19_03_20/tb_log`\u001b[39m\n",
       "\u001b[36m  To load the agent and statistic info:\u001b[39m\n",
       "\u001b[36m  ```\u001b[39m\n",
       "\u001b[36m  agent = RLCore.load(\"/run/media/cavani/cavani_hdx/julia-abc/workspace/checkpoints/JuliaRL_BasicDQN_CartPole_2020_09_02_19_03_20\", Agent)\u001b[39m\n",
       "\u001b[36m  BSON.@load joinpath(\"/run/media/cavani/cavani_hdx/julia-abc/workspace/checkpoints/JuliaRL_BasicDQN_CartPole_2020_09_02_19_03_20\", \"stats.bson\") total_reward_per_episode time_per_step\u001b[39m\n",
       "\u001b[36m  ```\u001b[39m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Experiment\n",
       "├─ agent => Agent\n",
       "│  ├─ policy => QBasedPolicy\n",
       "│  │  ├─ learner => BasicDQNLearner\n",
       "│  │  │  ├─ approximator => NeuralNetworkApproximator\n",
       "│  │  │  │  ├─ model => Flux.Chain\n",
       "│  │  │  │  │  └─ layers\n",
       "│  │  │  │  │     ├─ 1\n",
       "│  │  │  │  │     │  └─ Flux.Dense\n",
       "│  │  │  │  │     │     ├─ W => 128×4 Array{Float32,2}\n",
       "│  │  │  │  │     │     ├─ b => 128-element Array{Float32,1}\n",
       "│  │  │  │  │     │     └─ σ => typeof(NNlib.relu)\n",
       "│  │  │  │  │     ├─ 2\n",
       "│  │  │  │  │     │  └─ Flux.Dense\n",
       "│  │  │  │  │     │     ├─ W => 128×128 Array{Float32,2}\n",
       "│  │  │  │  │     │     ├─ b => 128-element Array{Float32,1}\n",
       "│  │  │  │  │     │     └─ σ => typeof(NNlib.relu)\n",
       "│  │  │  │  │     └─ 3\n",
       "│  │  │  │  │        └─ Flux.Dense\n",
       "│  │  │  │  │           ├─ W => 2×128 Array{Float32,2}\n",
       "│  │  │  │  │           ├─ b => 2-element Array{Float32,1}\n",
       "│  │  │  │  │           └─ σ => typeof(identity)\n",
       "│  │  │  │  └─ optimizer => Flux.Optimise.ADAM\n",
       "│  │  │  │     ├─ eta => 0.001\n",
       "│  │  │  │     ├─ beta\n",
       "│  │  │  │     │  ├─ 1\n",
       "│  │  │  │     │  │  └─ 0.9\n",
       "│  │  │  │     │  └─ 2\n",
       "│  │  │  │     │     └─ 0.999\n",
       "│  │  │  │     └─ state => IdDict\n",
       "│  │  │  │        ├─ ht => 32-element Array{Any,1}\n",
       "│  │  │  │        ├─ count => 6\n",
       "│  │  │  │        └─ ndel => 0\n",
       "│  │  │  ├─ loss_func => typeof(huber_loss)\n",
       "│  │  │  ├─ γ => 0.99\n",
       "│  │  │  ├─ batch_size => 32\n",
       "│  │  │  ├─ min_replay_history => 100\n",
       "│  │  │  ├─ rng => Random.MersenneTwister\n",
       "│  │  │  └─ loss => 0.12589972\n",
       "│  │  └─ explorer => EpsilonGreedyExplorer\n",
       "│  │     ├─ ϵ_stable => 0.01\n",
       "│  │     ├─ ϵ_init => 1.0\n",
       "│  │     ├─ warmup_steps => 0\n",
       "│  │     ├─ decay_steps => 500\n",
       "│  │     ├─ step => 10095\n",
       "│  │     ├─ rng => Random.MersenneTwister\n",
       "│  │     └─ is_training => true\n",
       "│  ├─ trajectory => CombinedTrajectory\n",
       "│  │  ├─ reward => 1000-element CircularArrayBuffer{Float32,1}\n",
       "│  │  ├─ terminal => 1000-element CircularArrayBuffer{Bool,1}\n",
       "│  │  ├─ state => 4×1000 view(::CircularArrayBuffer{Float32,2}, :, 1:1000) with eltype Float32\n",
       "│  │  ├─ next_state => 4×1000 view(::CircularArrayBuffer{Float32,2}, :, 2:1001) with eltype Float32\n",
       "│  │  ├─ full_state => 4×1001 view(::CircularArrayBuffer{Float32,2}, :, 1:1001) with eltype Float32\n",
       "│  │  ├─ action => 1000-element view(::CircularArrayBuffer{Int64,1}, 1:1000) with eltype Int64\n",
       "│  │  ├─ next_action => 1000-element view(::CircularArrayBuffer{Int64,1}, 2:1001) with eltype Int64\n",
       "│  │  └─ full_action => 1001-element view(::CircularArrayBuffer{Int64,1}, 1:1001) with eltype Int64\n",
       "│  ├─ role => DEFAULT_PLAYER\n",
       "│  └─ is_training => true\n",
       "├─ env => CartPoleEnv: SingleAgent(),Sequential(),PerfectInformation(),Deterministic(),StepReward(),GeneralSum(),MinimalActionSet(),Observation{Array}()\n",
       "├─ stop_condition => StopAfterStep\n",
       "│  ├─ step => 10000\n",
       "│  ├─ cur => 10001\n",
       "│  └─ progress => ProgressMeter.Progress\n",
       "├─ hook => ComposedHook\n",
       "│  └─ hooks\n",
       "│     ├─ 1\n",
       "│     │  └─ TotalRewardPerEpisode\n",
       "│     │     ├─ rewards => 94-element Array{Float64,1}\n",
       "│     │     └─ reward => 91.0\n",
       "│     ├─ 2\n",
       "│     │  └─ TimePerStep\n",
       "│     │     ├─ times => 100-element CircularArrayBuffer{Float64,1}\n",
       "│     │     └─ t => 89950690925965\n",
       "│     ├─ 3\n",
       "│     │  └─ DoEveryNStep\n",
       "│     │     ├─ f => ReinforcementLearningZoo.var\"#136#141\"\n",
       "│     │     ├─ n => 1\n",
       "│     │     └─ t => 10000\n",
       "│     ├─ 4\n",
       "│     │  └─ DoEveryNEpisode\n",
       "│     │     ├─ f => ReinforcementLearningZoo.var\"#138#143\"\n",
       "│     │     ├─ n => 1\n",
       "│     │     └─ t => 94\n",
       "│     └─ 5\n",
       "│        └─ DoEveryNStep\n",
       "│           ├─ f => ReinforcementLearningZoo.var\"#140#145\"\n",
       "│           ├─ n => 10000\n",
       "│           └─ t => 10000\n",
       "└─ description => \"    This experiment uses three dense layers to approximate the Q value....\"\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using ReinforcementLearning\n",
    "\n",
    "run(E`JuliaRL_BasicDQN_CartPole`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.1",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
