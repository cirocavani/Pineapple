{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder\n",
    "\n",
    "https://github.com/FluxML/model-zoo/blob/master/vision/mnist/vae.jl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Incompatibility detected between CUDA and LLVM 8.0+; disabling debug info emission for CUDA kernels\n",
      "└ @ CUDAnative /home/cavani/Workspace/julia-abc/software/julia-env/packages/CUDAnative/hfulr/src/CUDAnative.jl:114\n"
     ]
    }
   ],
   "source": [
    "using Flux\n",
    "using CuArrays\n",
    "using Images\n",
    "using Statistics\n",
    "import Random: seed!\n",
    "\n",
    "CuArrays.allowscalar(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600-element Array{CuArray{Float32,2,Nothing},1}:\n",
       " [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0]\n",
       " [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0]\n",
       " [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0]\n",
       " [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0]\n",
       " [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0]\n",
       " [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0]\n",
       " [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0]\n",
       " [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0]\n",
       " [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0]\n",
       " [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0]\n",
       " [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0]\n",
       " [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0]\n",
       " [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0]\n",
       " ⋮\n",
       " [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0]\n",
       " [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0]\n",
       " [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0]\n",
       " [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0]\n",
       " [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0]\n",
       " [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0]\n",
       " [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0]\n",
       " [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0]\n",
       " [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0]\n",
       " [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0]\n",
       " [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0]\n",
       " [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data, binarise it, and partition into mini-batches of M.\n",
    "\n",
    "import Flux.Data.MNIST\n",
    "\n",
    "function load_mnist_binary(; use_gpu=true)\n",
    "    X = float.(hcat(vec.(MNIST.images())...)) .> 0.5\n",
    "    if use_gpu\n",
    "        X = gpu(X)\n",
    "    end\n",
    "    return X\n",
    "end\n",
    "\n",
    "function make_batches(X, batch_size)\n",
    "    num_examples = size(X, 2)\n",
    "    data = [X[:, i] for i in Iterators.partition(1:num_examples, batch_size)]\n",
    "    return data\n",
    "end\n",
    "\n",
    "X = load_mnist_binary()\n",
    "M = 100\n",
    "data = make_batches(X, M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 60000)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 100)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size(data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 100)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size(data[end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784×10 CuArray{Float32,2,Nothing}:\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " ⋮                        ⋮                   \n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function sample_X(X, sample_size)\n",
    "    indices = rand(1:size(X, 2), sample_size)\n",
    "    X̃ = X[:, indices]\n",
    "    return X̃\n",
    "end\n",
    "\n",
    "X̃′ = sample_X(X, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################# Define Model #################################\n",
    "\n",
    "import Flux: functor\n",
    "\n",
    "e = Float32(ℯ)\n",
    "\n",
    "# Components of recognition model / \"encoder\" MLP.\n",
    "struct Encoder\n",
    "    A\n",
    "    μ\n",
    "    logσ\n",
    "end\n",
    "\n",
    "functor(g::Encoder) = (g.A, g.μ, g.logσ), y -> Encoder(y...)\n",
    "\n",
    "function (g::Encoder)(X)\n",
    "    A, μ, logσ = g.A, g.μ, g.logσ\n",
    "    h = A(X)\n",
    "    return μ(h), logσ(h)\n",
    "end\n",
    "\n",
    "function make_encoder(example_size, hidden_size, encoded_size; use_gpu=true)\n",
    "    A = Dense(example_size, hidden_size, tanh)\n",
    "    μ = Dense(hidden_size, encoded_size)\n",
    "    logσ = Dense(hidden_size, encoded_size)\n",
    "    \n",
    "    if use_gpu\n",
    "        A = gpu(A)\n",
    "        μ = gpu(μ)\n",
    "        logσ = gpu(logσ)\n",
    "    end\n",
    "    \n",
    "    encoder = Encoder(A, μ, logσ)\n",
    "    \n",
    "    return encoder\n",
    "end\n",
    "\n",
    "# Generative model / \"decoder\" MLP.\n",
    "function make_decoder(example_size, hidden_size, encoded_size; use_gpu=true)\n",
    "    decoder = Chain(\n",
    "        Dense(encoded_size, hidden_size, tanh),\n",
    "        Dense(hidden_size, example_size, σ)\n",
    "    )\n",
    "    \n",
    "    if use_gpu\n",
    "        decoder = gpu(decoder)\n",
    "    end\n",
    "    \n",
    "    return decoder\n",
    "end\n",
    "\n",
    "function z(μ, logσ; use_gpu=true)\n",
    "    noise = randn(Float32, size(logσ)...)\n",
    "    if use_gpu\n",
    "        noise = gpu(noise)\n",
    "    end\n",
    "    return μ + (e .^ logσ) .* noise\n",
    "end\n",
    "\n",
    "# Image size, Latent dimensionality, # hidden units.\n",
    "Dx = 28 ^ 2\n",
    "Dh = 500\n",
    "Dz = 5\n",
    "\n",
    "g = make_encoder(Dx, Dh, Dz)\n",
    "f = make_decoder(Dx, Dh, Dz);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : (500, 784)\n",
      "2 : (500,)\n",
      "3 : (5, 500)\n",
      "4 : (5,)\n",
      "5 : (5, 500)\n",
      "6 : (5,)\n"
     ]
    }
   ],
   "source": [
    "θg = params(g)\n",
    "\n",
    "for (i, θg′) in enumerate(θg)\n",
    "    println(i, \" : \", size(θg′))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : (500, 5)\n",
      "2 : (500,)\n",
      "3 : (784, 500)\n",
      "4 : (784,)\n"
     ]
    }
   ],
   "source": [
    "θf = params(f)\n",
    "\n",
    "for (i, θf′) in enumerate(θf)\n",
    "    println(i, \" : \", size(θf′))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x₁    : (784, 100)\n",
      "μ₁    : (5, 100)\n",
      "logσ₁ : (5, 100)\n"
     ]
    }
   ],
   "source": [
    "x₁ = data[1]\n",
    "μ₁, logσ₁ = g(x₁)\n",
    "\n",
    "println(\"x₁    : \", size(x₁))\n",
    "println(\"μ₁    : \", size(μ₁))\n",
    "println(\"logσ₁ : \", size(logσ₁))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z₁    : (5, 100)\n"
     ]
    }
   ],
   "source": [
    "z₁ = z(μ₁, logσ₁)\n",
    "\n",
    "println(\"z₁    : \", size(z₁))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x₁′   : (784, 100)\n"
     ]
    }
   ],
   "source": [
    "x₁′ = f(z₁)\n",
    "\n",
    "println(\"x₁′   : \", size(x₁′))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25150895\n"
     ]
    }
   ],
   "source": [
    "mse₁ = Flux.mse(x₁′, x₁)\n",
    "\n",
    "println(mse₁)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "548.37317f0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####################### Define ways of doing things with the model. #######################\n",
    "\n",
    "import Flux: binarycrossentropy\n",
    "\n",
    "# log p(x|z) - conditional probability of data given latents.\n",
    "log_p_x_z(x, z) = -sum(binarycrossentropy.(f(z), x))\n",
    "\n",
    "# KL-divergence between approximation posterior and N(0, 1) prior.\n",
    "kl_q_p(μ, logσ) = 0.5f0 * sum(e .^ (2f0 .* logσ) + μ .^ 2 .- 1f0 .- (2 .* logσ))\n",
    "\n",
    "# Monte Carlo estimator of mean ELBO using M samples.\n",
    "function L̄(X)\n",
    "    μ′, logσ′ = g(X)\n",
    "    z′ = z(μ′, logσ′)\n",
    "    (log_p_x_z(X, z′) - kl_q_p(μ′, logσ′)) * 1 // M\n",
    "end\n",
    "\n",
    "seed!(42)\n",
    "g = make_encoder(Dx, Dh, Dz)\n",
    "f = make_decoder(Dx, Dh, Dz)\n",
    "X̃ = sample_X(X, M)\n",
    "\n",
    "-L̄(X̃) # 548.37317f0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      " 57.196344 seconds (73.82 M allocations: 3.592 GiB, 2.32% gc time)\n",
      "Avg Loss : 161.7681\n",
      " -> New best loss! Saving model out to vae_mnist.bson\n",
      "\n",
      "Epoch 2\n",
      " 10.403250 seconds (7.01 M allocations: 298.742 MiB, 1.26% gc time)\n",
      "Avg Loss : 150.1147\n",
      " -> New best loss! Saving model out to vae_mnist.bson\n",
      "\n",
      "Epoch 3\n",
      " 10.359345 seconds (7.01 M allocations: 298.834 MiB, 1.24% gc time)\n",
      "Avg Loss : 144.1504\n",
      " -> New best loss! Saving model out to vae_mnist.bson\n",
      "\n",
      "Epoch 4\n",
      " 10.376736 seconds (7.01 M allocations: 298.784 MiB, 1.34% gc time)\n",
      "Avg Loss : 140.1847\n",
      " -> New best loss! Saving model out to vae_mnist.bson\n",
      "\n",
      "Epoch 5\n",
      " 10.378286 seconds (7.01 M allocations: 298.803 MiB, 1.32% gc time)\n",
      "Avg Loss : 136.4023\n",
      " -> New best loss! Saving model out to vae_mnist.bson\n",
      "\n",
      "Epoch 6\n",
      " 10.371215 seconds (7.01 M allocations: 298.788 MiB, 1.24% gc time)\n",
      "Avg Loss : 134.4593\n",
      " -> New best loss! Saving model out to vae_mnist.bson\n",
      "\n",
      "Epoch 7\n",
      " 10.364986 seconds (7.01 M allocations: 298.877 MiB, 1.25% gc time)\n",
      "Avg Loss : 133.2099\n",
      " -> New best loss! Saving model out to vae_mnist.bson\n",
      "\n",
      "Epoch 8\n",
      " 10.358487 seconds (7.01 M allocations: 298.667 MiB, 1.25% gc time)\n",
      "Avg Loss : 132.9227\n",
      " -> New best loss! Saving model out to vae_mnist.bson\n",
      "\n",
      "Epoch 9\n",
      " 10.358787 seconds (7.01 M allocations: 298.876 MiB, 1.23% gc time)\n",
      "Avg Loss : 131.6003\n",
      " -> New best loss! Saving model out to vae_mnist.bson\n",
      "\n",
      "Epoch 10\n",
      " 10.380891 seconds (7.01 M allocations: 298.766 MiB, 1.34% gc time)\n",
      "Avg Loss : 131.5026\n",
      " -> New best loss! Saving model out to vae_mnist.bson\n",
      "\n",
      "Epoch 11\n",
      " 10.376821 seconds (7.01 M allocations: 298.871 MiB, 1.31% gc time)\n",
      "Avg Loss : 130.0662\n",
      " -> New best loss! Saving model out to vae_mnist.bson\n",
      "\n",
      "Epoch 12\n",
      " 10.353243 seconds (7.01 M allocations: 298.762 MiB, 1.24% gc time)\n",
      "Avg Loss : 130.3133\n",
      "\n",
      "Epoch 13\n",
      " 10.353741 seconds (7.01 M allocations: 299.414 MiB, 1.24% gc time)\n",
      "Avg Loss : 129.1844\n",
      " -> New best loss! Saving model out to vae_mnist.bson\n",
      "\n",
      "Epoch 14\n",
      " 10.362609 seconds (7.01 M allocations: 299.408 MiB, 1.25% gc time)\n",
      "Avg Loss : 128.607\n",
      " -> New best loss! Saving model out to vae_mnist.bson\n",
      "\n",
      "Epoch 15\n",
      " 10.381804 seconds (7.01 M allocations: 298.569 MiB, 1.24% gc time)\n",
      "Avg Loss : 128.9022\n",
      "\n",
      "Epoch 16\n",
      " 10.361649 seconds (7.01 M allocations: 298.635 MiB, 1.23% gc time)\n",
      "Avg Loss : 128.2516\n",
      " -> New best loss! Saving model out to vae_mnist.bson\n",
      "\n",
      "Epoch 17\n",
      " 10.372657 seconds (7.01 M allocations: 298.767 MiB, 1.29% gc time)\n",
      "Avg Loss : 127.6281\n",
      " -> New best loss! Saving model out to vae_mnist.bson\n",
      "\n",
      "Epoch 18\n",
      " 10.370317 seconds (7.01 M allocations: 298.800 MiB, 1.27% gc time)\n",
      "Avg Loss : 128.0186\n",
      "\n",
      "Epoch 19\n",
      " 10.361765 seconds (7.01 M allocations: 299.496 MiB, 1.24% gc time)\n",
      "Avg Loss : 127.5914\n",
      " -> New best loss! Saving model out to vae_mnist.bson\n",
      "\n",
      "Epoch 20\n",
      " 10.376344 seconds (7.01 M allocations: 299.198 MiB, 1.35% gc time)\n",
      "Avg Loss : 127.8908\n",
      "\n",
      "Epoch 21\n",
      " 10.364991 seconds (7.01 M allocations: 298.559 MiB, 1.27% gc time)\n",
      "Avg Loss : 128.1276\n",
      "\n",
      "Epoch 22\n",
      " 10.361233 seconds (7.01 M allocations: 298.650 MiB, 1.24% gc time)\n",
      "Avg Loss : 127.5423\n",
      " -> New best loss! Saving model out to vae_mnist.bson\n",
      "\n",
      "Epoch 23\n",
      " 10.378670 seconds (7.01 M allocations: 298.789 MiB, 1.33% gc time)\n",
      "Avg Loss : 127.6458\n",
      "\n",
      "Epoch 24\n",
      " 10.387555 seconds (7.01 M allocations: 299.353 MiB, 1.34% gc time)\n",
      "Avg Loss : 127.174\n",
      " -> New best loss! Saving model out to vae_mnist.bson\n",
      "\n",
      "Epoch 25\n",
      " 10.362752 seconds (7.01 M allocations: 299.360 MiB, 1.24% gc time)\n",
      "Avg Loss : 127.2074\n",
      "\n",
      "Epoch 26\n",
      " 10.356035 seconds (7.01 M allocations: 298.604 MiB, 1.25% gc time)\n",
      "Avg Loss : 126.7927\n",
      " -> New best loss! Saving model out to vae_mnist.bson\n",
      "\n",
      "Epoch 27\n",
      " 10.396153 seconds (7.01 M allocations: 298.648 MiB, 1.31% gc time)\n",
      "Avg Loss : 126.8542\n",
      "\n",
      "Epoch 28\n",
      " 10.368838 seconds (7.01 M allocations: 298.892 MiB, 1.30% gc time)\n",
      "Avg Loss : 126.7805\n",
      " -> New best loss! Saving model out to vae_mnist.bson\n",
      "\n",
      "Epoch 29\n",
      " 10.365815 seconds (7.01 M allocations: 298.755 MiB, 1.27% gc time)\n",
      "Avg Loss : 126.9445\n",
      "\n",
      "Epoch 30\n",
      " 10.362153 seconds (7.01 M allocations: 299.333 MiB, 1.24% gc time)\n",
      "Avg Loss : 127.4485\n",
      "\n",
      "Epoch 31\n",
      " 10.360575 seconds (7.01 M allocations: 299.295 MiB, 1.25% gc time)\n",
      "Avg Loss : 126.8264\n",
      "\n",
      "Epoch 32\n",
      " 10.381180 seconds (7.01 M allocations: 298.526 MiB, 1.23% gc time)\n",
      "Avg Loss : 126.6992\n",
      " -> New best loss! Saving model out to vae_mnist.bson\n",
      "\n",
      "Epoch 33\n",
      " 10.407835 seconds (7.01 M allocations: 298.913 MiB, 1.31% gc time)\n",
      "Avg Loss : 126.54\n",
      " -> New best loss! Saving model out to vae_mnist.bson\n",
      "\n",
      "Epoch 34\n",
      " 10.368741 seconds (7.01 M allocations: 298.808 MiB, 1.27% gc time)\n",
      "Avg Loss : 126.9909\n",
      "\n",
      "Epoch 35\n",
      " 10.360614 seconds (7.01 M allocations: 299.330 MiB, 1.25% gc time)\n",
      "Avg Loss : 126.6896\n",
      "\n",
      "Epoch 36\n",
      " 10.362513 seconds (7.01 M allocations: 299.259 MiB, 1.27% gc time)\n",
      "Avg Loss : 126.2291\n",
      " -> New best loss! Saving model out to vae_mnist.bson\n",
      "\n",
      "Epoch 37\n",
      " 10.383141 seconds (7.01 M allocations: 298.789 MiB, 1.27% gc time)\n",
      "Avg Loss : 126.5285\n",
      "\n",
      "Epoch 38\n",
      " 10.365242 seconds (7.01 M allocations: 298.526 MiB, 1.23% gc time)\n",
      "Avg Loss : 126.1787\n",
      " -> New best loss! Saving model out to vae_mnist.bson\n",
      "\n",
      "Epoch 39\n",
      " 10.381733 seconds (7.01 M allocations: 298.874 MiB, 1.36% gc time)\n",
      "Avg Loss : 126.146\n",
      " -> New best loss! Saving model out to vae_mnist.bson\n",
      "\n",
      "Epoch 40\n",
      " 10.363648 seconds (7.01 M allocations: 298.842 MiB, 1.27% gc time)\n",
      "Avg Loss : 126.4762\n",
      "\n",
      "Epoch 41\n",
      " 10.366890 seconds (7.01 M allocations: 299.325 MiB, 1.26% gc time)\n",
      "Avg Loss : 126.0902\n",
      " -> New best loss! Saving model out to vae_mnist.bson\n",
      "\n",
      "Epoch 42\n",
      " 10.365407 seconds (7.01 M allocations: 299.472 MiB, 1.26% gc time)\n",
      "Avg Loss : 126.0934\n",
      "\n",
      "Epoch 43\n",
      " 10.361562 seconds (7.01 M allocations: 298.497 MiB, 1.26% gc time)\n",
      "Avg Loss : 125.6887\n",
      " -> New best loss! Saving model out to vae_mnist.bson\n",
      "\n",
      "Epoch 44\n",
      " 10.356312 seconds (7.01 M allocations: 298.769 MiB, 1.26% gc time)\n",
      "Avg Loss : 125.7956\n",
      "\n",
      "Epoch 45\n",
      " 10.363014 seconds (7.01 M allocations: 298.485 MiB, 1.26% gc time)\n",
      "Avg Loss : 126.1089\n",
      "\n",
      "Epoch 46\n",
      " 10.360067 seconds (7.01 M allocations: 299.428 MiB, 1.26% gc time)\n",
      "Avg Loss : 125.7394\n",
      "\n",
      "Epoch 47\n",
      " 10.361665 seconds (7.01 M allocations: 299.261 MiB, 1.26% gc time)\n",
      "Avg Loss : 125.8403\n",
      "\n",
      "Epoch 48\n",
      " 10.367505 seconds (7.01 M allocations: 298.555 MiB, 1.27% gc time)\n",
      "Avg Loss : 126.5476\n",
      " -> Haven't improved in a while, dropping learning rate to 0.0001!\n",
      "\n",
      "Epoch 49\n",
      " 10.364335 seconds (7.01 M allocations: 298.893 MiB, 1.30% gc time)\n",
      "Avg Loss : 123.1134\n",
      " -> New best loss! Saving model out to vae_mnist.bson\n",
      "\n",
      "Epoch 50\n",
      " 10.389409 seconds (7.01 M allocations: 298.822 MiB, 1.34% gc time)\n",
      "Avg Loss : 123.083\n",
      " -> New best loss! Saving model out to vae_mnist.bson\n",
      "\n",
      "Epoch 51\n",
      " 10.373531 seconds (7.01 M allocations: 298.772 MiB, 1.37% gc time)\n",
      "Avg Loss : 123.008\n",
      " -> New best loss! Saving model out to vae_mnist.bson\n",
      "\n",
      "Epoch 52\n",
      " 10.375575 seconds (7.01 M allocations: 298.806 MiB, 1.38% gc time)\n",
      "Avg Loss : 123.048\n",
      "\n",
      "Epoch 53\n",
      " 10.370526 seconds (7.01 M allocations: 299.457 MiB, 1.39% gc time)\n",
      "Avg Loss : 123.0098\n",
      "\n",
      "Epoch 54\n",
      " 10.362145 seconds (7.01 M allocations: 299.206 MiB, 1.29% gc time)\n",
      "Avg Loss : 122.9439\n",
      " -> New best loss! Saving model out to vae_mnist.bson\n",
      "\n",
      "Epoch 55\n",
      " 10.379421 seconds (7.01 M allocations: 298.797 MiB, 1.36% gc time)\n",
      "Avg Loss : 122.8933\n",
      " -> New best loss! Saving model out to vae_mnist.bson\n",
      "\n",
      "Epoch 56\n",
      " 10.413445 seconds (7.01 M allocations: 298.759 MiB, 1.28% gc time)\n",
      "Avg Loss : 122.8897\n",
      " -> New best loss! Saving model out to vae_mnist.bson\n",
      "\n",
      "Epoch 57\n",
      " 10.418982 seconds (7.01 M allocations: 298.778 MiB, 1.27% gc time)\n",
      "Avg Loss : 122.8817\n",
      " -> New best loss! Saving model out to vae_mnist.bson\n",
      "\n",
      "Epoch 58\n",
      " 10.431252 seconds (7.01 M allocations: 298.805 MiB, 1.32% gc time)\n",
      "Avg Loss : 122.8013\n",
      " -> New best loss! Saving model out to vae_mnist.bson\n",
      "\n",
      "Epoch 59\n",
      " 10.381440 seconds (7.01 M allocations: 298.773 MiB, 1.37% gc time)\n",
      "Avg Loss : 122.8504\n",
      "\n",
      "Epoch 60\n",
      " 10.427641 seconds (7.01 M allocations: 299.468 MiB, 1.37% gc time)\n",
      "Avg Loss : 122.8026\n",
      "\n",
      "Epoch 61\n",
      " 10.419713 seconds (7.01 M allocations: 299.242 MiB, 1.38% gc time)\n",
      "Avg Loss : 122.764\n",
      " -> New best loss! Saving model out to vae_mnist.bson\n",
      "\n",
      "Epoch 62\n",
      " 10.377780 seconds (7.01 M allocations: 298.647 MiB, 1.37% gc time)\n",
      "Avg Loss : 122.788\n",
      "\n",
      "Epoch 63\n",
      " 10.431328 seconds (7.01 M allocations: 298.624 MiB, 1.37% gc time)\n",
      "Avg Loss : 122.7928\n",
      "\n",
      "Epoch 64\n",
      " 10.428778 seconds (7.01 M allocations: 298.722 MiB, 1.40% gc time)\n",
      "Avg Loss : 122.784\n",
      "\n",
      "Epoch 65\n",
      " 10.414084 seconds (7.01 M allocations: 299.364 MiB, 1.27% gc time)\n",
      "Avg Loss : 122.7306\n",
      " -> New best loss! Saving model out to vae_mnist.bson\n",
      "\n",
      "Epoch 66\n",
      " 10.418679 seconds (7.01 M allocations: 299.465 MiB, 1.29% gc time)\n",
      "Avg Loss : 122.7389\n",
      "\n",
      "Epoch 67\n",
      " 10.471203 seconds (7.01 M allocations: 298.566 MiB, 1.26% gc time)\n",
      "Avg Loss : 122.7042\n",
      " -> New best loss! Saving model out to vae_mnist.bson\n",
      "\n",
      "Epoch 68\n",
      " 10.418254 seconds (7.01 M allocations: 298.797 MiB, 1.30% gc time)\n",
      "Avg Loss : 122.6518\n",
      " -> New best loss! Saving model out to vae_mnist.bson\n",
      "\n",
      "Epoch 69\n",
      " 10.424576 seconds (7.01 M allocations: 298.715 MiB, 1.30% gc time)\n",
      "Avg Loss : 122.6686\n",
      "\n",
      "Epoch 70\n",
      " 10.418610 seconds (7.01 M allocations: 299.388 MiB, 1.27% gc time)\n",
      "Avg Loss : 122.654\n",
      "\n",
      "Epoch 71\n",
      " 10.468504 seconds (7.01 M allocations: 299.393 MiB, 1.28% gc time)\n",
      "Avg Loss : 122.6618\n",
      "\n",
      "Epoch 72\n",
      " 10.465731 seconds (7.01 M allocations: 298.573 MiB, 1.27% gc time)\n",
      "Avg Loss : 122.641\n",
      " -> New best loss! Saving model out to vae_mnist.bson\n",
      "\n",
      "Epoch 73\n",
      " 10.371847 seconds (7.01 M allocations: 298.659 MiB, 1.33% gc time)\n",
      "Avg Loss : 122.6441\n",
      "\n",
      "Epoch 74\n",
      " 10.520097 seconds (7.01 M allocations: 298.762 MiB, 1.28% gc time)\n",
      "Avg Loss : 122.603\n",
      " -> New best loss! Saving model out to vae_mnist.bson\n",
      "\n",
      "Epoch 75\n",
      " 10.429837 seconds (7.01 M allocations: 298.781 MiB, 1.32% gc time)\n",
      "Avg Loss : 122.6197\n",
      "\n",
      "Epoch 76\n",
      " 10.430289 seconds (7.01 M allocations: 299.465 MiB, 1.29% gc time)\n",
      "Avg Loss : 122.6319\n",
      "\n",
      "Epoch 77\n",
      " 10.461623 seconds (7.01 M allocations: 299.169 MiB, 1.27% gc time)\n",
      "Avg Loss : 122.6133\n",
      "\n",
      "Epoch 78\n",
      " 10.457237 seconds (7.01 M allocations: 298.721 MiB, 1.28% gc time)\n",
      "Avg Loss : 122.6305\n",
      "\n",
      "Epoch 79\n",
      " 10.481738 seconds (7.01 M allocations: 298.827 MiB, 1.35% gc time)\n",
      "Avg Loss : 122.5388\n",
      " -> New best loss! Saving model out to vae_mnist.bson\n",
      "\n",
      "Epoch 80\n",
      " 10.485595 seconds (7.01 M allocations: 298.799 MiB, 1.42% gc time)\n",
      "Avg Loss : 122.5809\n",
      "\n",
      "Epoch 81\n",
      " 10.475014 seconds (7.01 M allocations: 298.800 MiB, 1.41% gc time)\n",
      "Avg Loss : 122.5382\n",
      " -> New best loss! Saving model out to vae_mnist.bson\n",
      "\n",
      "Epoch 82\n",
      " 10.430502 seconds (7.01 M allocations: 298.754 MiB, 1.33% gc time)\n",
      "Avg Loss : 122.5342\n",
      " -> New best loss! Saving model out to vae_mnist.bson\n",
      "\n",
      "Epoch 83\n",
      " 10.480584 seconds (7.01 M allocations: 298.933 MiB, 1.40% gc time)\n",
      "Avg Loss : 122.5315\n",
      " -> New best loss! Saving model out to vae_mnist.bson\n",
      "\n",
      "Epoch 84\n",
      " 10.750087 seconds (7.01 M allocations: 298.761 MiB, 1.66% gc time)\n",
      "Avg Loss : 122.515\n",
      " -> New best loss! Saving model out to vae_mnist.bson\n",
      "\n",
      "Epoch 85\n",
      " 10.758359 seconds (7.01 M allocations: 298.840 MiB, 1.49% gc time)\n",
      "Avg Loss : 122.5335\n",
      "\n",
      "Epoch 86\n",
      " 10.748377 seconds (7.01 M allocations: 298.842 MiB, 1.46% gc time)\n",
      "Avg Loss : 122.5245\n",
      "\n",
      "Epoch 87\n",
      " 10.744392 seconds (7.01 M allocations: 298.806 MiB, 1.45% gc time)\n",
      "Avg Loss : 122.5183\n",
      "\n",
      "Epoch 88\n",
      " 10.755417 seconds (7.01 M allocations: 298.733 MiB, 1.46% gc time)\n",
      "Avg Loss : 122.4799\n",
      " -> New best loss! Saving model out to vae_mnist.bson\n",
      "\n",
      "Epoch 89\n",
      " 10.744920 seconds (7.01 M allocations: 298.817 MiB, 1.51% gc time)\n",
      "Avg Loss : 122.4912\n",
      "\n",
      "Epoch 90\n",
      " 10.754237 seconds (7.01 M allocations: 298.847 MiB, 1.48% gc time)\n",
      "Avg Loss : 122.4722\n",
      " -> New best loss! Saving model out to vae_mnist.bson\n",
      "\n",
      "Epoch 91\n",
      " 10.764982 seconds (7.01 M allocations: 298.778 MiB, 1.50% gc time)\n",
      "Avg Loss : 122.482\n",
      "\n",
      "Epoch 92\n",
      " 10.748795 seconds (7.01 M allocations: 298.889 MiB, 1.47% gc time)\n",
      "Avg Loss : 122.4463\n",
      " -> New best loss! Saving model out to vae_mnist.bson\n",
      "\n",
      "Epoch 93\n",
      " 10.715194 seconds (7.01 M allocations: 298.823 MiB, 1.53% gc time)\n",
      "Avg Loss : 122.5178\n",
      "\n",
      "Epoch 94\n",
      " 10.804865 seconds (7.01 M allocations: 298.842 MiB, 1.47% gc time)\n",
      "Avg Loss : 122.4346\n",
      " -> New best loss! Saving model out to vae_mnist.bson\n",
      "\n",
      "Epoch 95\n",
      " 10.716074 seconds (7.01 M allocations: 298.733 MiB, 1.51% gc time)\n",
      "Avg Loss : 122.4588\n",
      "\n",
      "Epoch 96\n",
      " 10.761468 seconds (7.01 M allocations: 298.827 MiB, 1.49% gc time)\n",
      "Avg Loss : 122.4783\n",
      "\n",
      "Epoch 97\n",
      " 10.757388 seconds (7.01 M allocations: 298.795 MiB, 1.47% gc time)\n",
      "Avg Loss : 122.4222\n",
      " -> New best loss! Saving model out to vae_mnist.bson\n",
      "\n",
      "Epoch 98\n",
      " 10.725884 seconds (7.01 M allocations: 298.842 MiB, 1.53% gc time)\n",
      "Avg Loss : 122.4447\n",
      "\n",
      "Epoch 99\n",
      " 10.832562 seconds (7.01 M allocations: 298.806 MiB, 1.55% gc time)\n",
      "Avg Loss : 122.3882\n",
      " -> New best loss! Saving model out to vae_mnist.bson\n",
      "\n",
      "Epoch 100\n",
      " 10.843128 seconds (7.01 M allocations: 298.866 MiB, 1.70% gc time)\n",
      "Avg Loss : 122.4244\n",
      "\n",
      "Epoch 101\n",
      " 10.838178 seconds (7.01 M allocations: 298.796 MiB, 1.67% gc time)\n",
      "Avg Loss : 122.3644\n",
      " -> New best loss! Saving model out to vae_mnist.bson\n",
      "\n",
      "Epoch 102\n",
      " 10.721387 seconds (7.01 M allocations: 298.781 MiB, 1.54% gc time)\n",
      "Avg Loss : 122.3789\n",
      "\n",
      "Epoch 103\n",
      " 10.762349 seconds (7.01 M allocations: 298.799 MiB, 1.50% gc time)\n",
      "Avg Loss : 122.3991\n",
      "\n",
      "Epoch 104\n",
      " 10.807946 seconds (7.01 M allocations: 298.866 MiB, 1.48% gc time)\n",
      "Avg Loss : 122.4143\n",
      "\n",
      "Epoch 105\n",
      " 10.831396 seconds (7.01 M allocations: 298.779 MiB, 1.67% gc time)\n",
      "Avg Loss : 122.4\n",
      "\n",
      "Epoch 106\n",
      " 10.834248 seconds (7.01 M allocations: 298.820 MiB, 1.68% gc time)\n",
      "Avg Loss : 122.3992\n",
      " -> Haven't improved in a while, dropping learning rate to 1.0e-5!\n",
      "\n",
      "Epoch 107\n",
      " 10.810589 seconds (7.01 M allocations: 298.821 MiB, 1.49% gc time)\n",
      "Avg Loss : 122.117\n",
      " -> New best loss! Saving model out to vae_mnist.bson\n",
      "\n",
      "Epoch 108\n",
      " 10.781027 seconds (7.01 M allocations: 298.782 MiB, 1.50% gc time)\n",
      "Avg Loss : 122.1033\n",
      " -> New best loss! Saving model out to vae_mnist.bson\n",
      "\n",
      "Epoch 109\n",
      " 10.968491 seconds (7.01 M allocations: 298.728 MiB, 1.60% gc time)\n",
      "Avg Loss : 122.081\n",
      " -> New best loss! Saving model out to vae_mnist.bson\n",
      "\n",
      "Epoch 110\n",
      " 11.022762 seconds (7.01 M allocations: 298.775 MiB, 1.61% gc time)\n",
      "Avg Loss : 122.0973\n",
      "\n",
      "Epoch 111\n",
      " 11.014597 seconds (7.01 M allocations: 298.827 MiB, 1.63% gc time)\n",
      "Avg Loss : 122.0854\n",
      "\n",
      "Epoch 112\n",
      " 11.170834 seconds (7.01 M allocations: 298.755 MiB, 1.61% gc time)\n",
      "Avg Loss : 122.0868\n",
      "\n",
      "Epoch 113\n",
      " 11.078098 seconds (7.01 M allocations: 298.817 MiB, 1.57% gc time)\n",
      "Avg Loss : 122.0759\n",
      " -> New best loss! Saving model out to vae_mnist.bson\n",
      "\n",
      "Epoch 114\n",
      " 10.743091 seconds (7.01 M allocations: 298.870 MiB, 1.52% gc time)\n",
      "Avg Loss : 122.096\n",
      "\n",
      "Epoch 115\n",
      " 10.812308 seconds (7.01 M allocations: 298.755 MiB, 1.50% gc time)\n",
      "Avg Loss : 122.0882\n",
      "\n",
      "Epoch 116\n",
      " 10.810292 seconds (7.01 M allocations: 298.755 MiB, 1.49% gc time)\n",
      "Avg Loss : 122.0808\n",
      "\n",
      "Epoch 117\n",
      " 10.940918 seconds (7.01 M allocations: 298.889 MiB, 1.57% gc time)\n",
      "Avg Loss : 122.0856\n",
      "\n",
      "Epoch 118\n",
      " 10.957191 seconds (7.01 M allocations: 298.731 MiB, 1.58% gc time)\n",
      "Avg Loss : 122.0705\n",
      " -> New best loss! Saving model out to vae_mnist.bson\n",
      "\n",
      "Epoch 119\n",
      " 10.942467 seconds (7.01 M allocations: 298.889 MiB, 1.65% gc time)\n",
      "Avg Loss : 122.0715\n",
      "\n",
      "Epoch 120\n",
      " 10.940107 seconds (7.01 M allocations: 298.805 MiB, 1.54% gc time)\n",
      "Avg Loss : 122.0796\n",
      "\n",
      "Epoch 121\n",
      " 10.813207 seconds (7.01 M allocations: 298.839 MiB, 1.50% gc time)\n",
      "Avg Loss : 122.0678\n",
      " -> New best loss! Saving model out to vae_mnist.bson\n",
      "\n",
      "Epoch 122\n",
      " 10.843741 seconds (7.01 M allocations: 298.754 MiB, 1.59% gc time)\n",
      "Avg Loss : 122.0885\n",
      "\n",
      "Epoch 123\n",
      " 10.811928 seconds (7.01 M allocations: 298.872 MiB, 1.52% gc time)\n",
      "Avg Loss : 122.0762\n",
      "\n",
      "Epoch 124\n",
      " 10.941655 seconds (7.01 M allocations: 298.733 MiB, 1.55% gc time)\n",
      "Avg Loss : 122.0644\n",
      " -> New best loss! Saving model out to vae_mnist.bson\n",
      "\n",
      "Epoch 125\n",
      " 10.823972 seconds (7.01 M allocations: 298.862 MiB, 1.58% gc time)\n",
      "Avg Loss : 122.0663\n",
      "\n",
      "Epoch 126\n",
      " 10.770280 seconds (7.01 M allocations: 298.847 MiB, 1.51% gc time)\n",
      "Avg Loss : 122.0708\n",
      "\n",
      "Epoch 127\n",
      " 10.799718 seconds (7.01 M allocations: 298.778 MiB, 1.71% gc time)\n",
      "Avg Loss : 122.0773\n",
      "\n",
      "Epoch 128\n",
      " 10.880053 seconds (7.01 M allocations: 298.822 MiB, 1.69% gc time)\n",
      "Avg Loss : 122.0802\n",
      "\n",
      "Epoch 129\n",
      " 10.829665 seconds (7.01 M allocations: 298.889 MiB, 1.62% gc time)\n",
      "Avg Loss : 122.0574\n",
      " -> New best loss! Saving model out to vae_mnist.bson\n",
      "\n",
      "Epoch 130\n",
      " 10.782923 seconds (7.01 M allocations: 298.687 MiB, 1.62% gc time)\n",
      "Avg Loss : 122.0654\n",
      "\n",
      "Epoch 131\n",
      " 10.843770 seconds (7.01 M allocations: 298.866 MiB, 1.72% gc time)\n",
      "Avg Loss : 122.0724\n",
      "\n",
      "Epoch 132\n",
      " 11.004187 seconds (7.01 M allocations: 298.871 MiB, 1.79% gc time)\n",
      "Avg Loss : 122.0695\n",
      "\n",
      "Epoch 133\n",
      " 10.842492 seconds (7.01 M allocations: 298.751 MiB, 1.70% gc time)\n",
      "Avg Loss : 122.0764\n",
      "\n",
      "Epoch 134\n",
      " 10.841344 seconds (7.01 M allocations: 298.820 MiB, 1.71% gc time)\n",
      "Avg Loss : 122.0418\n",
      " -> New best loss! Saving model out to vae_mnist.bson\n",
      "\n",
      "Epoch 135\n",
      " 10.802219 seconds (7.01 M allocations: 298.827 MiB, 1.75% gc time)\n",
      "Avg Loss : 122.0502\n",
      "\n",
      "Epoch 136\n",
      " 10.788826 seconds (7.01 M allocations: 298.755 MiB, 1.72% gc time)\n",
      "Avg Loss : 122.0504\n",
      "\n",
      "Epoch 137\n",
      " 10.836580 seconds (7.01 M allocations: 298.817 MiB, 1.72% gc time)\n",
      "Avg Loss : 122.0705\n",
      "\n",
      "Epoch 138\n",
      " 10.843365 seconds (7.01 M allocations: 298.759 MiB, 1.71% gc time)\n",
      "Avg Loss : 122.0474\n",
      "\n",
      "Epoch 139\n",
      " 10.849263 seconds (7.01 M allocations: 298.911 MiB, 1.70% gc time)\n",
      "Avg Loss : 122.0684\n",
      " -> Haven't improved in a while, dropping learning rate to 1.0000000000000002e-6!\n",
      "\n",
      "Epoch 140\n",
      " 10.846739 seconds (7.01 M allocations: 298.755 MiB, 1.73% gc time)\n",
      "Avg Loss : 122.0619\n",
      "\n",
      "Epoch 141\n",
      " 10.984077 seconds (7.01 M allocations: 298.868 MiB, 1.69% gc time)\n",
      "Avg Loss : 122.0507\n",
      "\n",
      "Epoch 142\n",
      " 10.778426 seconds (7.01 M allocations: 298.820 MiB, 1.52% gc time)\n",
      "Avg Loss : 122.0331\n",
      " -> New best loss! Saving model out to vae_mnist.bson\n",
      "\n",
      "Epoch 143\n",
      " 10.697626 seconds (7.01 M allocations: 298.822 MiB, 1.57% gc time)\n",
      "Avg Loss : 122.0353\n",
      "\n",
      "Epoch 144\n",
      " 10.775747 seconds (7.01 M allocations: 298.850 MiB, 1.52% gc time)\n",
      "Avg Loss : 122.0521\n",
      "\n",
      "Epoch 145\n",
      " 10.851138 seconds (7.01 M allocations: 298.751 MiB, 1.56% gc time)\n",
      "Avg Loss : 122.0396\n",
      "\n",
      "Epoch 146\n",
      " 10.900898 seconds (7.01 M allocations: 298.887 MiB, 1.76% gc time)\n",
      "Avg Loss : 122.0518\n",
      "\n",
      "Epoch 147\n",
      " 10.853073 seconds (7.01 M allocations: 298.806 MiB, 1.74% gc time)\n",
      "Avg Loss : 122.0546\n",
      " -> Haven't improved in a while, dropping learning rate to 1.0000000000000002e-7!\n",
      "\n",
      "Epoch 148\n",
      " 10.846298 seconds (7.01 M allocations: 298.800 MiB, 1.74% gc time)\n",
      "Avg Loss : 122.0371\n",
      "\n",
      "Epoch 149\n",
      " 10.865304 seconds (7.01 M allocations: 298.884 MiB, 1.73% gc time)\n",
      "Avg Loss : 122.0519\n",
      "\n",
      "Epoch 150\n",
      " 10.905867 seconds (7.01 M allocations: 298.759 MiB, 1.62% gc time)\n",
      "Avg Loss : 122.0373\n",
      "\n",
      "Epoch 151\n",
      " 10.560950 seconds (7.01 M allocations: 298.778 MiB, 1.38% gc time)\n",
      "Avg Loss : 122.0419\n",
      "\n",
      "Epoch 152\n",
      " 10.484472 seconds (7.01 M allocations: 298.777 MiB, 1.35% gc time)\n",
      "Avg Loss : 122.0444\n",
      "\n",
      "Epoch 153\n",
      " 10.485483 seconds (7.01 M allocations: 298.912 MiB, 1.35% gc time)\n",
      "Avg Loss : 122.0462\n",
      "\n",
      "Epoch 154\n",
      " 10.538960 seconds (7.01 M allocations: 298.731 MiB, 1.38% gc time)\n",
      "Avg Loss : 122.0604\n",
      "\n",
      "Epoch 155\n",
      " 10.480926 seconds (7.01 M allocations: 298.933 MiB, 1.35% gc time)\n",
      "Avg Loss : 122.0345\n",
      "\n",
      "Epoch 156\n",
      " 10.483240 seconds (7.01 M allocations: 298.761 MiB, 1.37% gc time)\n",
      "Avg Loss : 122.0394\n",
      "\n",
      "Epoch 157\n",
      " 10.923718 seconds (7.01 M allocations: 298.929 MiB, 1.75% gc time)\n",
      "Avg Loss : 122.0331\n",
      " -> We're calling this converged.\n"
     ]
    }
   ],
   "source": [
    "################################# Learn Parameters ##############################\n",
    "\n",
    "import Flux: train!\n",
    "import BSON: bson\n",
    "\n",
    "θ = params(g, f)\n",
    "optimizer = ADAM()\n",
    "L2_f() = sum(x->sum(x .^ 2), params(f))\n",
    "loss(X) = -L̄(X) + 0.01f0 * L2_f()\n",
    "\n",
    "num_epochs = 200\n",
    "early_stopping_patience = 10\n",
    "learning_rate_schedule = 5\n",
    "\n",
    "best_loss = nothing\n",
    "last_improvement = 0\n",
    "\n",
    "for epoch_idx in 1:num_epochs\n",
    "    println(\"Epoch \", epoch_idx)\n",
    "    @time train!(loss, θ, zip(data), optimizer)\n",
    "    avg_loss = mean([-L̄(x) for x in data])\n",
    "\n",
    "    println(\"Avg Loss : \", round(avg_loss; digits=4))\n",
    "\n",
    "    # If this is the best metric we've seen so far, save the model out\n",
    "    if best_loss === nothing || avg_loss ≤ best_loss\n",
    "        println(\" -> New best loss! Saving model out to vae_mnist.bson\")\n",
    "        bson(\"vae_mnist.bson\", encoder = cpu(g), decoder = cpu(f), epoch = epoch_idx, loss = avg_loss)\n",
    "        best_loss = avg_loss\n",
    "        last_improvement = epoch_idx\n",
    "    end\n",
    "\n",
    "    # If we haven't seen improvement in N epochs, drop our learning rate:\n",
    "    if epoch_idx - last_improvement ≥ learning_rate_schedule && optimizer.eta > 1e-6\n",
    "        optimizer.eta /= 10.0\n",
    "        println(\" -> Haven't improved in a while, dropping learning rate to $(optimizer.eta)!\")\n",
    "\n",
    "        # After dropping learning rate, give it a few epochs to improve\n",
    "        last_improvement = epoch_idx\n",
    "    end\n",
    "\n",
    "    if epoch_idx - last_improvement ≥ early_stopping_patience\n",
    "        println(\" -> We're calling this converged.\")\n",
    "        break\n",
    "    end\n",
    "    \n",
    "    println()\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjAAAAA4AQAAAADLRv+GAAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAAmJLR0QAAd2KE6QAAAJfSURBVEjH7ZbBai4hDIWFbIW8iuBWyKsLsxXmVYTZCt7xnAydwr3tXbgp1EWtGj8nJzH+Ify2n9NS+takf7VYd2JE0MU3+WOxftpBI7W1En1Oat2IsULTTA99cUzM6rEmNMQ3TI91sk0/iR5vwhxjYjQbMJOm14DRBf2nS0z3wzm4ofD4tBEjNmF1/10bz9MxxM1JL1xh4vLCSMkcydiICeWEVtaxeNbOLwY0jHZ3qfVG/dsrDNm19dzchBkHxB3Vg8pAAxq0KkyeDfUV9ui5kcZGzP1puv7pCb4lA00TstG6cAe3FIb4Kjy+v7JgF2bcxmu/poVpdO1MGN327SPSWv37GX+li5I2YrQnYJow4XgJcj3X5KzwVHg149NBd7lc6r4Xo3PltufWhdkSVhreDjMbuT+lvnqphjm/sFI3YqTHssZ2cjgdgw/vYSwBs+IIC8xJUaIp8dyLEYhrXhMMsxN1UlqG/CXPhA6qTgNGr/xKyW0YT7nJm5o5KIbPN/hoDYsHK1SjxGYeop2YO9ooysZCHepSMxqckcF4T9ZZQcHQefBOIEHDuHZiZFaUaD0vBnAJl6fgge2GHdnKMo4RcT8PdBKMSqOobsNMqKSHPy3r/ktjTZqK9DuYDHLgoKEJNFYvHe7UFkyYJ5STyHHiLIuRUcbnFjLExloSLjrVtmKKsV4lmgb4lgYq9fU4yo77teARigdC7T9rdmEib0G8XHJ0hW74k5b8JRRnNkYagOcH6CaMiyr+wmbS/BfZV+2Q+sHehvHmr5eGfwPi52H6m+UmzNO+9+a/2ibMb/P2B/nSNPcFU3mbAAAAAElFTkSuQmCC",
      "text/plain": [
       "28×280 Array{Gray{N0f8},2} with eltype Gray{Normed{UInt8,8}}:\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " ⋮                                 ⋱                   \n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjAAAAA4AQAAAADLRv+GAAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAAmJLR0QAAd2KE6QAAAGSSURBVEjH7ZbBbgQhCIZNvG7iC03Cq/NIJr2SUNcfHGy23UOZpofhwqrwjSDilnLLLX8ktX8/+i2m6mbDUC2aNt5MmkS/KpmYph1KTpNSSAONXmGacgwqCUOqQTXbKkYFbN0xD1h+2GwqRlmDelRG3mZah5pr7m+aMOhXYJBBxWIBpvWJGQqGHBzGdPSvmZhRPfT8IRuGGBiOHn4JmOI2sjEtYlD4BE1+D/q5ND60YzgZIxODY6WI0R2DmOr2QVNZmA4M2cVH4QtW5TQqVnUnRq/BzMZDuIULQ1YMAXN4bPGDZeU7A9O+YqapzhCH97S1JiIe24bRKzDWRHEzRtcCBp1yjGbCba1aS+9WMIkYLz/H1A1DjnnOdsP4AxPfgDzMXHVMEWRT5tpq9+HcxTD9uABjO65smOPEjGjsZcOp1glTxFb5AoyfpkVjmKNZpjnUpN3CFaFawWRiyHccU4yDXk+xNSxzWNm4AGN/TIpKoLn/K1nZkDPQNIyL0Sw2KW+F+qvZJIw3bH4P+BGeg7nlX8snhYwOmafBF8MAAAAASUVORK5CYII=",
      "text/plain": [
       "28×280 Array{Gray{N0f8},2} with eltype Gray{Normed{UInt8,8}}:\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " ⋮                                 ⋱                   \n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "################################# Sample Output ##############################\n",
    "\n",
    "import Distributions: Bernoulli, mode\n",
    "\n",
    "# Sample from the learned model.\n",
    "function model_sample_dist(; use_gpu=true)\n",
    "    μ = zeros(Dz)\n",
    "    logσ = zeros(Dz)\n",
    "    if use_gpu\n",
    "        μ = gpu(μ)\n",
    "        logσ = gpu(logσ)\n",
    "    end\n",
    "    z′ = z(μ, logσ)\n",
    "    x′ = f(z′)\n",
    "    return Bernoulli.(cpu(x′))\n",
    "end\n",
    "\n",
    "img(x) = Gray.(reshape(x, 28, 28))\n",
    "\n",
    "image_samples = [model_sample_dist() for i = 1:10]\n",
    "\n",
    "display(hcat(img.(map(x -> rand.(x), image_samples))...))\n",
    "\n",
    "display(hcat(img.(map(x -> mode.(x), image_samples))...))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.0-rc2",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
